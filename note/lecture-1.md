#### ​**1. 强化学习（RL）与监督学习的核心差异**​

- ​**数据非独立同分布（Non-iid）​**​  
    强化学习中，数据具有时序相关性，当前状态和动作会影响后续状态（马尔可夫决策过程）。例如，在机器人导航中，向左转的动作会导致后续观测到不同的环境状态，数据之间高度依赖。
    
    - ​**解决方法**​：经验回放（Experience Replay）技术通过存储历史交互数据并随机采样，打破时序相关性。
- ​**无显式标签，仅通过奖励反馈**​  
    监督学习依赖标注数据（如分类标签），而RL仅通过稀疏的奖励信号（如游戏胜利/失败）进行学习。例如，训练AI玩《超级马里奥》时，仅在通关或死亡时获得奖励，中间步骤需自主探索有效策略。
#### ​**2. RL基本范式：Agent-Environment交互**​

- ​**核心要素**​：
    - ​**动作（Action）​**​：Agent对环境施加的影响（如机器人移动关节）。
    - ​**奖励（Reward）​**​：环境反馈的标量信号，指导Agent优化策略（如到达目标+1，碰撞-1）。
- ​**交互流程**​：  
    Agent观察状态（State）→ 选择动作 → 环境更新状态并返回奖励 → Agent调整策略以最大化累积奖励（Return）。
#### ​**3. 核心思想：感知与行为系统的端到端结合**​

- ​**直接优化行为策略**​：  
    RL关注最终行为效果，而非中间特征表示。例如，自动驾驶系统直接处理摄像头输入，输出转向/刹车指令，无需显式识别道路标志。
- ​**深度学习的角色**​：  
    神经网络自动提取感知层特征（如CNN处理图像），并与策略网络（输出动作）联合优化，实现端到端训练。
#### ​**4. 超越稀疏奖励的监督方法**​

- ​**模仿学习（Imitation Learning）​**​
    
    - ​**行为克隆（Behavior Clustering）​**​：直接复制专家动作（如通过人类演示数据训练自动驾驶）。
    - ​**逆强化学习（Inverse RL）​**​：从专家行为反推奖励函数，再基于此优化策略（如推断人类驾驶的潜在安全偏好）。
- ​**无监督学习辅助RL**​
    
    - ​**预测模型（Model-Based RL）​**​：学习环境动力学模型，预测状态转移与奖励，辅助规划（如AlphaGo的蒙特卡洛树搜索）。
    - ​**自监督预训练**​：通过预测未来状态或解耦表征（如对比学习）提升策略泛化性。
- ​**迁移学习**​  
    复用已有任务的知识加速新任务学习（如将在模拟器中训练的机器人策略迁移到真实世界）。
#### ​**5. 单一算法的普适性挑战**​

- ​**现状**​：现有算法（如PPO、SAC）需针对任务调参，尚无“通用AI”。
- ​**难点**​：
    - ​**样本效率**​：复杂任务需海量交互数据（如机器人控制）。
    - ​**探索-利用权衡**​：稀疏奖励下需平衡尝试新动作与执行已知策略。
    - ​**任务异构性**​：不同任务的状态/动作空间差异大（如Atari游戏 vs. 机械臂控制）。
- ​**前沿方向**​：元强化学习（Meta-RL）让Agent学会快速适应新任务，迈向通用性。