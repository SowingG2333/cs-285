# Paper: [[Ghosh 等 - 2020 - Learning to Reach Goals via Iterated Supervised Learning.pdf]]

![[Pasted image 20250417134206.png]]

---

# Summary

这篇论文提出了一种名为"目标条件监督学习(GCSL)"的新型强化学习算法，用于解决目标导向任务中的稀疏奖励问题。以下是核心要点总结：

1. 问题背景：
- 传统RL算法（如TD学习和策略梯度）在稀疏奖励场景下存在训练不稳定、超参数敏感等问题
- 模仿学习虽然稳定，但需要专家演示数据

2. 核心创新：
- 提出"事后重标记(hindsight relabeling)"技术：将任何轨迹重新解释为对其最终到达状态的"成功演示"
- 通过迭代的监督学习过程，将自主收集的次优轨迹转化为训练数据

3. 算法特点：
- 无需价值函数估计，仅通过策略的监督学习实现目标导向行为
- 支持完全自监督学习，也可兼容专家演示数据
- 理论证明优化了RL目标的下界

4. 实验验证：
- 在5个基准任务（导航、机械臂操作等）上超越或匹配TD3-HER和PPO算法
- 相比价值函数方法具有更好的超参数鲁棒性
- 能更有效地利用演示数据加速学习

5. 优势：
- 实现简单（仅需策略网络）
- 训练稳定
- 数据效率高（可重用所有历史数据）
- 兼容离线和在线学习该工作展示了如何通过巧妙的轨迹重标记技术，将模仿学习的稳定性与强化学习的自主探索能力相结合，为稀疏奖励场景提供了一种简单有效的解决方案。

---

# Q&A

- 我可不可以理解为这篇文章构建了一个agent：**自主生成数据——标注数据——训练数据**的一个系统？因为每次agent进行决策都会生成一个action，然后我们结合条件状态使得该action正确，接着再利用条件状态输入训练这次的策略。
- 是的，你的理解基本正确。这篇文章提出的**目标条件监督学习(GCSL)**确实构建了一个自主生成-标注-训练的闭环系统，其核心机制可以分解为以下三个关键步骤：
	1. **自主生成数据**  
	    Agent在当前策略π(a|s,g,h)指导下与环境交互，收集可能次优的轨迹τ = {s₁,a₁,...,s_T}（目标g可能未达成。初期包含10000步的随机探索以丰富数据多样性。
	2. **事后重标记数据**  
	    通过**hindsight relabeling**技术，将轨迹τ中的每个状态s_{t+h}重新定义为"目标"，并将对应动作a_t标记为"正确动作"，生成专家数据元组(s_t, a_t, s_{t+h}, h)。这种重标记使得任何轨迹都能被重新解释为对实际到达状态的"成功演示"。
	3. **监督学习训练**  
	    使用最大似然估计（行为克隆）在所有历史数据（包括新重标记数据）上训练策略网络。网络输入为状态-目标联合特征（400+300神经元的两层MLP），输出离散化动作概率。
		该系统的创新性在于：
		- **无需价值函数**：完全通过监督学习实现强化学习目标，避免了TD误差或策略梯度的不稳定性
		- **数据高效利用**：所有历史轨迹都被保留并持续用于训练（通过重标记保持其最优性）
		- **理论保证**：证明该过程优化了RL目标的下界，当重标记数据比例趋近于0时，边界紧度最高值得注意的是，虽然流程类似模仿学习，但GCSL的关键区别在于： "GCSL使用agent自身从零开始收集的数据进行训练，而非来自专家或（带噪声的）最优监督者的数据" 这种自我迭代的改进过程使得初始次优策略能逐步收敛到最优行为，其有效性已通过理论分析和机器人实验验证。